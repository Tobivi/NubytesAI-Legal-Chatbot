{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8577032,"sourceType":"datasetVersion","datasetId":5128995},{"sourceId":9047315,"sourceType":"datasetVersion","datasetId":5454810},{"sourceId":9354120,"sourceType":"datasetVersion","datasetId":5670554},{"sourceId":9355043,"sourceType":"datasetVersion","datasetId":5671214}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.41.3 trl==0.4.7 huggingface_hub==0.23.0 transformers==4.41.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T18:20:21.379444Z","iopub.execute_input":"2024-09-09T18:20:21.379823Z","iopub.status.idle":"2024-09-09T18:20:21.384492Z","shell.execute_reply.started":"2024-09-09T18:20:21.379796Z","shell.execute_reply":"2024-09-09T18:20:21.383353Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:13:44.800520Z","iopub.execute_input":"2024-10-14T10:13:44.800777Z","iopub.status.idle":"2024-10-14T10:13:44.811406Z","shell.execute_reply.started":"2024-10-14T10:13:44.800754Z","shell.execute_reply":"2024-10-14T10:13:44.810601Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl\n%pip install sentence-transformers\n%pip install faiss","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:13:50.186157Z","iopub.execute_input":"2024-10-14T10:13:50.186825Z","iopub.status.idle":"2024-10-14T10:15:26.290456Z","shell.execute_reply.started":"2024-10-14T10:13:50.186794Z","shell.execute_reply":"2024-10-14T10:15:26.289401Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token= \"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\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:15:53.710099Z","iopub.execute_input":"2024-10-14T10:15:53.710516Z","iopub.status.idle":"2024-10-14T10:15:54.170684Z","shell.execute_reply.started":"2024-10-14T10:15:53.710482Z","shell.execute_reply":"2024-10-14T10:15:54.169779Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging,TextIteratorStreamer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom threading import Thread","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:15:58.081426Z","iopub.execute_input":"2024-10-14T10:15:58.082025Z","iopub.status.idle":"2024-10-14T10:16:16.063505Z","shell.execute_reply.started":"2024-10-14T10:15:58.081995Z","shell.execute_reply":"2024-10-14T10:16:16.062733Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-10-14 10:16:04.325937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-14 10:16:04.326035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-14 10:16:04.455990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n# secret_value_hf= user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_value_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:18:53.803893Z","iopub.execute_input":"2024-10-14T10:18:53.805113Z","iopub.status.idle":"2024-10-14T10:18:53.970673Z","shell.execute_reply.started":"2024-10-14T10:18:53.805081Z","shell.execute_reply":"2024-10-14T10:18:53.969941Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"base_model = 'mistralai/Mistral-7B-Instruct-v0.2'\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n#################################################################\n# bitsandbytes parameters\n#################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n#################################################################\n# Set up quantization config\n#################################################################\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n#################################################################\n# Load pre-trained config\n#################################################################\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:19:00.451093Z","iopub.execute_input":"2024-10-14T10:19:00.451936Z","iopub.status.idle":"2024-10-14T10:20:24.438947Z","shell.execute_reply.started":"2024-10-14T10:19:00.451903Z","shell.execute_reply":"2024-10-14T10:20:24.438200Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfaa4a46ade841d2b6a78e3ca269ef8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee2b9b29a8b4caea103eaffcabee4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fed4be0cc54b519a6b9765bea74fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d8e155fe854e75bfff0f03544f199b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c02915a9aa4c8ba797a833d3109011"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a076442a69a64556a9fccd383756884e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce46c8f86eba4a839f9315d5bdc39f9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef27b012c4ea41d7aa7ffd684c117fde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90ffcfd982c4231910c3821b142c6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc933eb14a44edd8a027c39d5d93cff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61bd271a144446f82b58c6c4850f43f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5489cfda96f448b8fd29d1b746ee7d8"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:20:40.112973Z","iopub.execute_input":"2024-10-14T10:20:40.113347Z","iopub.status.idle":"2024-10-14T10:20:40.119219Z","shell.execute_reply.started":"2024-10-14T10:20:40.113311Z","shell.execute_reply":"2024-10-14T10:20:40.118338Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_path = \"/kaggle/input/final-dataset/merged_output2.json\"\ndataset = load_dataset('json', data_files=dataset_path)\n\nprint(dataset)\ndata_split = dataset[\"train\"]\nif 'instruction' in data_split.column_names:\n    print(data_split[\"instruction\"][:10])\nelse:\n    print(\"The 'instruction' column does not exist in the dataset.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:26:14.423026Z","iopub.execute_input":"2024-10-14T10:26:14.423874Z","iopub.status.idle":"2024-10-14T10:26:14.830373Z","shell.execute_reply.started":"2024-10-14T10:26:14.423840Z","shell.execute_reply":"2024-10-14T10:26:14.829372Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response'],\n        num_rows: 138188\n    })\n})\n['1-800 Flowers Inc v Phonenames Ltd Neutral Citation Number: [2001] EWCA Civ 721| ||Case No: A3 2000 0052 CHANCF| IN THE SUPREME COURT OF JUDICATURE COURT OF APPEAL (CIVIL DIVISION) ON APPEAL FROM THE HIGH COURT, CHANCERY DIVISION (Mr Justice Jacob) ||Royal Courts of Justice Strand, London, WC2A 2LL| ||Thursday 17th May 2001| B e f o r e : LORD JUSTICE PETER GIBSON LORD JUSTICE BUXTON and LORD JUSTICE JONATHAN PARKER ____________________ |1-800 FLOWERS INC|Applicant| |- and -|| |PHONENAMES LTD|Opponent| ____________________ (Transcript of the Handed Down Judgment of Smith Bernal Reporting Limited, 190 Fleet Street London EC4A 2AG Tel No: 020 7421 4040, Fax No: 020 7831 8838 Official Shorthand Writers to the Court) ____________________ Mr Geoffrey Hobbs QC and Miss Emma Himsworth (instructed by Messrs Lupton Fawcett for the Applicant) Mr Mark Platts-Mills QC and Mr James Abrahams (instructed by Messrs Field Fisher Waterhouse for the Opponent) ____________________ HTML VERSION OF JUDGMENT', '2525 Resource Management Consulting Ltd v Steel Neutral Citation Number: [2002] EWCA Civ 129| ||A1/2001/1961/1961A| IN THE SUPREME COURT OF JUDICATURE IN THE COURT OF APPEAL (CIVIL DIVISION) ON APPEAL FROM AN EMPLOYMENT APPEAL TRIBUNAL (Mr Justice Howell QC) ||Royal Courts of Justice Strand London WC2 Tuesday, 29th January 2002| ||| B e f o r e : LORD JUSTICE MUMMERY LORD JUSTICE DYSON ____________________ |2525 RESOURCE MANAGEMENT CONSULTING LTD|| |Appellant/Applicant|| |- v -|| |KEVIN STEEL|| |Respondent|| ____________________ (Computer Aided Transcript of the Palantype Notes of Smith Bernal Reporting Limited, 190 Fleet Street, London EC4A 2AG Tel: 0171 421 4040 Official Shorthand Writers to the Court) ____________________ MR NIGEL GAUDER and MR MARTIN GILLIBRAND appeared in person. The Respondent did not appear and was not represented ____________________ HTML VERSION OF JUDGMENT', '7 Strathray Gardens Ltd. v Pointstar Shipping & Finance Ltd & Anor [2004] EWCA Civ 1669 (15 December 2004)', \"A B & Ors v Liverpool City Council & Ors IN THE SUPREME COURT OF JUDICATURE LTA 98/5058/1 5059/1 COURT OF APPEAL (CIVIL DIVISION) 5078/1 5089/1 5090/1 ON APPEAL FROM THE HIGH COURT OF JUSTICE QUEEN'S BENCH DIVISION MANCHESTER DISTRICT REGISTRY Royal Courts of Justice Strand, London WC2\", 'A v Secretary of State for the Home Department [2004] EWCA Civ 255 (16 February 2004)', 'A (A Child), Re [2004] EWCA Civ 641 (07 May 2004)', 'A & Ors v Secretary of State for the Home Department [2004] EWCA Civ 1123 (11 August 2004)', 'A (A Child), Re Neutral Citation Number: [2002] EWCA Civ 1313| ||B1/2002/0937| IN THE SUPREME COURT OF JUDICATURE IN THE COURT OF APPEAL (CIVIL DIVISION) ON APPEAL FROM THE HIGH COURT OF JUSTICE FAMILY DIVISION (Her Honour Judge Darwall-Smith) ||The Royal Courts of Justice Strand London WC2 Monday 29th July, 2002| ||| B e f o r e : LORD JUSTICE THORPE LORD JUSTICE CLARKE MR JUSTICE FERRIS ____________________ |A (A CHILD)|| ____________________ (Computer-aided transcript of the Palantype Notes of Smith Bernal Reporting Limited 190 Fleet Street London EC4A 2AG Tel: 020 7404 1400 Official Shorthand Writers to the Court) ____________________ MR E ALDERGILL (instructed by Messrs Leon Kaye Solicitors, London SW11 1QU) appeared on behalf of the Respondent ____________________ HTML VERSION OF JUDGMENT', 'A-N (A Child), Re Neutral Citation Number: [2002] EWCA Civ 1013| ||B1/2001/2851| IN THE SUPREME COURT OF JUDICATURE COURT OF APPEAL (CIVIL DIVISION) ON APPEAL FROM LEEDS COUNTY COURT (HIS HONOUR JUDGE BUSH) ||Royal Courts of Justice Strand London WC2', 'A (Children), Re Neutral Citation Number: [2002] EWCA Civ 1343| ||B1/2002/0602| IN THE SUPREME COURT OF JUDICATURE IN THE COURT OF APPEAL (CIVIL DIVISION) ON APPEAL FROM THE IPSWICH COUNTY COURT (His Honour Judge Thompson) ||The Royal Courts of Justice Strand London WC2 Friday 26th July, 2002| ||| B e f o r e : LORD JUSTICE THORPE MR JUSTICE FERRIS ____________________ |A (CHILDREN)|| ____________________ (Computer-aided transcript of the Palantype Notes of Smith Bernal Reporting Limited 190 Fleet Street London EC4A 2AG Tel: 020 7404 1400 Official Shorthand Writers to the Court) ____________________ MR B JUBB (instructed by Messrs Harter and Loveless) appeared on behalf of the Appellant THE RESPONDENT appeared on his own behalf ____________________ HTML VERSION OF JUDGMENT']\n","output_type":"stream"}]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:26:18.986687Z","iopub.execute_input":"2024-10-14T10:26:18.987462Z","iopub.status.idle":"2024-10-14T10:26:20.444637Z","shell.execute_reply.started":"2024-10-14T10:26:18.987427Z","shell.execute_reply":"2024-10-14T10:26:20.443827Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:26:27.013987Z","iopub.execute_input":"2024-10-14T10:26:27.014389Z","iopub.status.idle":"2024-10-14T10:26:27.056969Z","shell.execute_reply.started":"2024-10-14T10:26:27.014360Z","shell.execute_reply":"2024-10-14T10:26:27.056074Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=data_split,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"instruction\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:26:30.389406Z","iopub.execute_input":"2024-10-14T10:26:30.390009Z","iopub.status.idle":"2024-10-14T10:26:42.747834Z","shell.execute_reply.started":"2024-10-14T10:26:30.389979Z","shell.execute_reply":"2024-10-14T10:26:42.747077Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/138188 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115676f2768941cea5a8c7b11fda61a4"}},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:28:12.425931Z","iopub.execute_input":"2024-10-14T10:28:12.426598Z","iopub.status.idle":"2024-10-14T10:28:12.459655Z","shell.execute_reply.started":"2024-10-14T10:28:12.426566Z","shell.execute_reply":"2024-10-14T10:28:12.458651Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"login(token = \"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\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:39:44.285183Z","iopub.execute_input":"2024-10-14T10:39:44.285830Z","iopub.status.idle":"2024-10-14T10:39:44.399024Z","shell.execute_reply.started":"2024-10-14T10:39:44.285797Z","shell.execute_reply":"2024-10-14T10:39:44.398121Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"repo_name = \"NubytesAI\"\nusername = \"Tobivictor\" \nrepo_id = f\"{username}/{repo_name}\"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:40:13.505614Z","iopub.execute_input":"2024-10-14T10:40:13.506670Z","iopub.status.idle":"2024-10-14T10:40:13.510941Z","shell.execute_reply.started":"2024-10-14T10:40:13.506633Z","shell.execute_reply":"2024-10-14T10:40:13.509896Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer.model.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:40:18.667813Z","iopub.execute_input":"2024-10-14T10:40:18.668613Z","iopub.status.idle":"2024-10-14T10:40:29.990713Z","shell.execute_reply.started":"2024-10-14T10:40:18.668580Z","shell.execute_reply":"2024-10-14T10:40:29.989857Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-670cf513-06e45878439571576c60787e;129d4407-ae3c-4549-8f93-969d8f64dc51)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.2 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248b862e0c404939825ee723afb7632c"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Tobivictor/NubytesAI/commit/bfaca22a988d6e3510798bc4beb7e5ebde7a7f22', commit_message='Upload model', commit_description='', oid='bfaca22a988d6e3510798bc4beb7e5ebde7a7f22', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tobivictor/NubytesAI', endpoint='https://huggingface.co', repo_type='model', repo_id='Tobivictor/NubytesAI'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"adapter_model = \"Tobivictor/NubytesAI\"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:40:53.503901Z","iopub.execute_input":"2024-10-14T10:40:53.504746Z","iopub.status.idle":"2024-10-14T10:40:53.508773Z","shell.execute_reply.started":"2024-10-14T10:40:53.504712Z","shell.execute_reply":"2024-10-14T10:40:53.507759Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# adapter_model = \"oluwatobinubytes/updated_model_NubytesAI\"\nbase_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model,quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:41:47.612207Z","iopub.execute_input":"2024-10-14T10:41:47.612599Z","iopub.status.idle":"2024-10-14T10:42:33.127219Z","shell.execute_reply.started":"2024-10-14T10:41:47.612570Z","shell.execute_reply":"2024-10-14T10:42:33.126506Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ffcf17b26f4b5a90415f2c396dc14c"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\nmodel = PeftModel.from_pretrained(model, adapter_model)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = model.to(\"cuda\")\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:43:02.245753Z","iopub.execute_input":"2024-10-14T10:43:02.246171Z","iopub.status.idle":"2024-10-14T10:43:05.987198Z","shell.execute_reply.started":"2024-10-14T10:43:02.246139Z","shell.execute_reply":"2024-10-14T10:43:05.986196Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:43:15.009424Z","iopub.execute_input":"2024-10-14T10:43:15.009790Z","iopub.status.idle":"2024-10-14T10:43:15.143247Z","shell.execute_reply.started":"2024-10-14T10:43:15.009760Z","shell.execute_reply":"2024-10-14T10:43:15.142272Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(repo_id,tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:43:18.364118Z","iopub.execute_input":"2024-10-14T10:43:18.364497Z","iopub.status.idle":"2024-10-14T10:43:20.353771Z","shell.execute_reply.started":"2024-10-14T10:43:18.364468Z","shell.execute_reply":"2024-10-14T10:43:20.352912Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f557844142c4400c8b06aefbee84136e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-670cf5c6-406d3cc8024f17fc32b5a273;46722a69-8360-4145-acdc-15de8ce18eff)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 to ask for access. - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.2.\n  warnings.warn(\nNo files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Tobivictor/NubytesAI/commit/bfaca22a988d6e3510798bc4beb7e5ebde7a7f22', commit_message='Upload model', commit_description='', oid='bfaca22a988d6e3510798bc4beb7e5ebde7a7f22', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tobivictor/NubytesAI', endpoint='https://huggingface.co', repo_type='model', repo_id='Tobivictor/NubytesAI'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:43:25.756162Z","iopub.execute_input":"2024-10-14T10:43:25.756581Z","iopub.status.idle":"2024-10-14T10:43:27.192135Z","shell.execute_reply.started":"2024-10-14T10:43:25.756550Z","shell.execute_reply":"2024-10-14T10:43:27.191261Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c9bb66506b489f85d310a827a5b50e"}},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Tobivictor/NubytesAI/commit/548c42e3805e9cf35d4b8cde416a391c4aa9ad29', commit_message='Upload tokenizer', commit_description='', oid='548c42e3805e9cf35d4b8cde416a391c4aa9ad29', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tobivictor/NubytesAI', endpoint='https://huggingface.co', repo_type='model', repo_id='Tobivictor/NubytesAI'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"def determine_prompt_type(content):\n    \"\"\"\n    Determine the prompt type based on the content.\n    This is a placeholder implementation. Adjust the logic as needed.\n    \"\"\"\n    if \"court case\" in content or \"v\" in content:\n        return \"case\"\n    elif \"compare\" in content:\n        return \"multi_jurisdiction\"\n    else:\n        return \"subject_matter\"\n\ndef generate_structured_response(content):\n    \"\"\"\n    Generate structured responses based on the determined prompt type.\n    It prepares multiple instructions to answer legal queries depending on the type of prompt.\n    \"\"\"\n    # Determine the type of prompt based on the input content\n    prompt_type = determine_prompt_type(content)\n    \n    # Define prompts based on the prompt type\n    if prompt_type == \"subject_matter\":\n        prompts = {\n            \"introduction\": f\"<s>[INST] Explain the concept of the subject matter in detail within the jurisdiction: {content} [/INST]\",\n            \"rule\": f\"<s>[INST] State the legal rules guiding those principles, mentioning statutes, US codes, cases, and judgments: {content} [/INST]\",\n            \"cases\": f\"<s>[INST] List and provide a detailed analysis of cases dealing with the subject matter in the jurisdiction, including issues, rules, and authority, and conclusions: {content} [/INST]\",\n            \"summary\": f\"<s>[INST] Summarize the subject matter based on the discussions above: {content} [/INST]\"\n        }\n    \n    elif prompt_type == \"case\":\n        prompts = {\n            \"issues\": f\"<s>[INST] Discuss the issue the case deals with: {content} [/INST]\",\n            \"case_journey\": f\"<s>[INST] Discuss the journey of the case, highlighting the courts it had been to and their decisions: {content} [/INST]\",\n            \"rules_authority\": f\"<s>[INST] State the rules and authority relied upon by the highest court in this case: {content} [/INST]\",\n            \"similar_cases\": f\"<s>[INST] Identify and analyze other cases dealing with similar legal issues or principles as those in the case provided. Provide detailed analyses including issues, rules, and conclusions: {content} [/INST]\",\n            \"conclusion\": f\"<s>[INST] Conclude by discussing the decision of the case: {content} [/INST]\"\n        }\n    \n    elif prompt_type == \"multi_jurisdiction\":\n        prompts = {\n            \"subject_matter\": f\"<s>[INST] Discuss the subject matter within each of the selected jurisdictions: {content} [/INST]\",\n            \"legal_rules\": f\"<s>[INST] Discuss the legal rules within each jurisdiction, citing statutes/codes and judgments: {content} [/INST]\",\n            \"comparison\": f\"<s>[INST] Summarize the discussion by comparing the subject matter across the jurisdictions: {content} [/INST]\"\n        }\n    \n    else:\n        raise ValueError(\"Unknown prompt type\")\n\n    # Generate the responses for each section of the structured prompts\n    results = {}\n    for key, prompt in prompts.items():\n        result = pipe(prompt, max_length=2000)[0]['generated_text']  # Adjust max_length if needed\n        results[key] = result\n    \n    return results\n\ncontent = \"What’s the position of law regarding death sentence in South Africa\"\nresponses = generate_structured_response(content)\n\nfor key, response in responses.items():\n    print(f'{key}: {response}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:54:47.363844Z","iopub.execute_input":"2024-10-14T10:54:47.364239Z","iopub.status.idle":"2024-10-14T11:02:05.857201Z","shell.execute_reply.started":"2024-10-14T10:54:47.364205Z","shell.execute_reply":"2024-10-14T11:02:05.856252Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"introduction: <s>[INST] Explain the concept of the subject matter in detail within the jurisdiction: What’s the position of law regarding death sentence in South Africa [/INST] In South Africa, the death penalty, also known as capital punishment, is a legal penalty that can be imposed by the courts for certain crimes. The use of the death penalty in South Africa has a complex and controversial history, and its current legal status is subject to ongoing debate.\n\nHistorically, the death penalty was a common punishment in South Africa, particularly during the apartheid era. However, following the end of apartheid and the establishment of a democratic government, there were significant changes to the legal framework governing the use of the death penalty.\n\nThe South African Constitution, which came into effect in 1994, prohibits the use of the death penalty for all crimes other than murder. This means that only those convicted of murder can be sentenced to death. The Constitution also sets out certain safeguards to ensure that the death penalty is only imposed in the most serious cases.\n\nOne of the key safeguards is the requirement for a mandatory review of all death sentences by the Supreme Court of Appeal. This means that anyone sentenced to death must have their sentence reviewed by the Supreme Court of Appeal, which has the power to confirm, quash, or reduce the sentence. This review process is intended to ensure that the death penalty is only imposed in cases where it is absolutely necessary and proportionate.\n\nAnother safeguard is the requirement for a unanimous decision by the judges in the trial court. This means that all judges in the trial court must agree on the imposition of the death penalty. This requirement is intended to ensure that the decision to impose the death penalty is not taken lightly or arbitrarily.\n\nDespite these safeguards, the use of the death penalty in South Africa remains a contentious issue. Some argue that it is an unnecessary and inhumane punishment, while others believe that it is an essential tool for deterring serious crime. There have been several high-profile cases in which the death penalty has been imposed, and there have also been several high-profile cases in which the death sentence has been quashed on review.\n\nIn recent years, there have been calls for the abolition of the death penalty in South Africa. Some argue that it is inconsistent with the country's commitment to human rights and the principles of dignity and compassion enshrined in the Constitution. Others argue that the death penalty is an expensive and ineffective form of punishment, and that resources would be better spent on rehabilitation and reintegration programs for offenders.\n\nDespite these arguments, the death penalty remains a legal penalty in South Africa, and it is up to the courts to determine whether it is an appropriate sentence in individual cases. The position of the law regarding the death penalty in South Africa is therefore complex and subject to ongoing debate and interpretation.\n\nrule: <s>[INST] State the legal rules guiding those principles, mentioning statutes, US codes, cases, and judgments: What’s the position of law regarding death sentence in South Africa [/INST] In South Africa, the legal rules regarding the imposition of the death sentence are governed by the Constitution of the Republic of South Africa, 1994, and various statutes. Here are the key provisions:\n\n1. Constitution of South Africa, 1994: The Constitution of South Africa, in Section 11(d), guarantees the right to life, which is a fundamental human right. However, the Constitution also allows for the death penalty in certain circumstances. Section 35(3)(b) of the Constitution permits the death penalty for crimes committed with aggravating circumstances as determined by an Act of Parliament.\n\n2. Criminal Law (Sentencing Act), 1995: This Act sets out the general principles of sentencing in South Africa, including the death penalty. Section 229 of the Act provides that the death penalty may be imposed for aggravated murder, defined as murder committed in the course of committing or intending to commit certain enumerated crimes, such as robbery, rape, or kidnapping.\n\n3. The Death Penalty for Rape and Sexual Offences Act, 1997: This Act amended the Criminal Law (Sentencing Act) to include the death penalty as a sentencing option for aggravated rape and sexual offenses. Section 3 of the Act states that the death penalty may be imposed for aggravated rape, defined as rape committed in the course of committing or intending to commit certain enumerated crimes, such as robbery, kidnapping, or the use of a firearm.\n\n4. Cases and Judgments: Several cases have addressed the application of the death penalty in South Africa. In State v Makwanyane (1995) 11 SA 391 (CC), the Constitutional Court held that the death penalty was constitutional, but only for the most serious crimes and only in exceptional cases. In State v Mthembo (2001) 1 SACR 1, the Constitutional Court held that the death penalty could not be imposed for crimes committed without the use of a firearm. In State v Mahlangu (2015) 37 ILJ 173 (CC), the Constitutional Court held that the death penalty could not be imposed for crimes committed without the intention to cause death.\n\nIt is important to note that the use of the death penalty in South Africa is a controversial issue, and there have been calls for its abolition. In 2016, the South African Cabinet announced that it would not impose the death penalty, and the President has not signed any death warrants since 1995. However, the death penalty remains a legal option for courts in South Africa.\n\ncases: <s>[INST] List and provide a detailed analysis of cases dealing with the subject matter in the jurisdiction, including issues, rules, and authority, and conclusions: What’s the position of law regarding death sentence in South Africa [/INST] In South Africa, the death penalty is a controversial and complex issue that has been the subject of extensive legal debate and constitutional interpretation. The following cases are significant in shaping the current legal landscape regarding the death sentence in South Africa:\n\n1. S v Makwanyane, S v Mchunu (1995) 3 SA 391 (CC): In this landmark case, the Constitutional Court of South Africa declared the death penalty unconstitutional under the interim Constitution. The Court held that the mandatory nature of the death penalty violated the right to life, which is a fundamental human right enshrined in the Constitution. The Court also found that the death penalty was an arbitrary and discriminatory punishment, as it was applied disproportionately to black offenders.\n\n2. S v Williams (1995) 6 BHRC 665 (CC): Following the Makwanyane judgment, the Constitutional Court considered the issue of the death penalty in the context of a specific case. In this case, the Court upheld the death sentence imposed on the accused, finding that the aggravating circumstances of the crime outweighed the mitigating circumstances. However, the Court emphasized that the death penalty was an exceptional measure that should only be imposed in the most serious cases.\n\n3. S v Mthembo (2004) 3 SSA 1 (CC): In this case, the Constitutional Court considered the issue of the death penalty in the context of a juvenile offender. The Court held that the death penalty was unconstitutional as a punishment for juvenile offenders, as it violated the constitutional protection of the right to life.\n\n4. S v Mahlangu (2015) ZACC 13: In this case, the Constitutional Court considered the issue of the death penalty in the context of a case where the accused was mentally impaired. The Court held that the death penalty was unconstitutional as a punishment for mentally impaired offenders, as it violated the constitutional protection of the right to human dignity.\n\n5. S v Dippenaar (2017) ZASCA 131: In this case, the Supreme Court of Appeal considered the issue of the death penalty in the context of a case where the accused was found to have committed a crime while under the influence of alcohol. The Court held that the death penalty was not an appropriate punishment in this case, as the accused's intoxication was a mitigating factor.\n\nIn conclusion, the position of the law regarding the death sentence in South Africa is that it is unconstitutional in certain circumstances, including for juvenile offenders, mentally impaired offenders, and in cases where the aggravating circumstances do not outweigh the mitigating circumstances. The death penalty is an exceptional measure that should only be imposed in the most serious cases, and the courts will carefully consider all the circumstances of the case before imposing the death sentence.\n\nsummary: <s>[INST] Summarize the subject matter based on the discussions above: What’s the position of law regarding death sentence in South Africa [/INST] The death penalty, or capital punishment, is a controversial issue in South Africa. The country has a complex legal history regarding the use of the death sentence. South Africa's Constitution, which came into effect in 1994, abolished the death penalty for all crimes, making South Africa a de facto abolitionist country. However, there are exceptions for certain crimes, such as treason, committed during the apartheid era, for which individuals may still be on death row.\n\nThe Constitutional Court of South Africa has ruled that the mandatory application of the death penalty is unconstitutional, meaning that it is up to the discretion of the judge in each individual case whether to impose the death sentence. The court has also set strict guidelines for the imposition of the death sentence, requiring that it be applied only in the most exceptional cases where there is no alternative but to impose the harshest punishment.\n\nThere have been ongoing debates and calls for the complete abolition of the death penalty in South Africa, with some arguing that it is an inhumane and irreversible punishment that does not deter crime. Others argue that it is necessary for the most heinous crimes, such as murder and treason, to ensure justice and protect society.\n\nIn recent years, there have been several high-profile cases in which the death sentence has been sought, including the trial of Oscar Pistorius for the murder of his girlfriend Reeva Steenkamp. The case drew international attention and sparked renewed debate about the use of the death penalty in South Africa. Ultimately, the court imposed a sentence of five years' imprisonment for Pistorius, with the possibility of parole after serving 10 years.\n\nIn conclusion, the position of law in South Africa regarding the death sentence is complex, with the Constitution having abolished the death penalty for all crimes but allowing for its application in certain exceptional cases. The use of the death penalty is subject to the discretion of the judge and strict guidelines set by the Constitutional Court. There are ongoing debates about the morality and effectiveness of the death penalty, with some calling for its complete abolition.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}